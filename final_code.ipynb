{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 FIT5149 \n",
    "\n",
    "### Group no. 45\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "aip7inINLpOD",
    "outputId": "2d27813f-54fc-4a88-c274-72b3cd52187b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/akshitchawla/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tokenize\n",
    "import textblob\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## KERAS\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import collections\n",
    "nltk.download(\"wordnet\")\n",
    "from textblob import Word\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "from sklearn import model_selection\n",
    "import re\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab setup and drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L03cKkMgSqmz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "errBmcxDSqm3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/akshitchawla/Documents/fit5149_new/Assignment 2/train_data.csv')\n",
    "#df_train = pd.read_csv('gdrive/FIT5149_Assignment_2/Assignment2/Training Dataset-20190429/train_data.csv')\n",
    "df_train_lb = pd.read_csv('/Users/akshitchawla/Documents/fit5149_new/Assignment 2/train_label.csv')\n",
    "# combining the train data and label based on trn_id\n",
    "df_full = df_train.merge(df_train_lb,on=['trn_id'], how='inner')\n",
    "test_data = pd.read_csv('/Users/akshitchawla/Documents/fit5149_new/Assignment 2/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratoring Data dimensions without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9ETqK1EoSqm5",
    "outputId": "38c09767-c3d1-4665-d5ee-2434cbfe25c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape # shape of the combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "nM8Tjj8xSqm7",
    "outputId": "6a82cd20-8491-491b-e295-d7acb410da5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 650000 entries, 0 to 649999\n",
      "Data columns (total 3 columns):\n",
      "trn_id    650000 non-null object\n",
      "text      650000 non-null object\n",
      "label     650000 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 19.8+ MB\n"
     ]
    }
   ],
   "source": [
    "## memory usage and other stats\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "JJARIkc_Sqm9",
    "outputId": "23936fb8-eaec-4d74-ba08-3c7d9b246b54",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    130000\n",
       "2    130000\n",
       "3    130000\n",
       "4    130000\n",
       "5    130000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the label distribution\n",
    "df_full['label'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "-- Sorry please scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcO7ZtRMnySN"
   },
   "outputs": [],
   "source": [
    "# contraction list declaration\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\",\n",
    "  \"aint\": \"am not\",\n",
    "  \"arent\": \"are not\",\n",
    "  \"cant\": \"cannot\",\n",
    "  \"cantve\": \"cannot have\",\n",
    "  \"cause\": \"because\",\n",
    "  \"couldve\": \"could have\",\n",
    "  \"couldnt\": \"could not\",\n",
    "  \"couldntve\": \"could not have\",\n",
    "  \"didnt\": \"did not\",\n",
    "  \"doesnt\": \"does not\",\n",
    "  \"dont\": \"do not\",\n",
    "  \"hadnt\": \"had not\",\n",
    "  \"hadntve\": \"had not have\",\n",
    "  \"hasnt\": \"has not\",\n",
    "  \"havent\": \"have not\",\n",
    "  \"hed\": \"he would\",\n",
    "  \"hedve\": \"he would have\",\n",
    "  \"hell\": \"he will\",\n",
    "  \"hellve\": \"he will have\",\n",
    "  \"hes\": \"he is\",\n",
    "  \"howd\": \"how did\",\n",
    "  \"howdy\": \"how do you\",\n",
    "  \"howll\": \"how will\",\n",
    "  \"hows\": \"how is\",\n",
    "  \"Id\": \"I would\",\n",
    "  \"Idve\": \"I would have\",\n",
    "  \"Ill\": \"I will\",\n",
    "  \"Illve\": \"I will have\",\n",
    "  \"Im\": \"I am\",\n",
    "  \"Ive\": \"I have\",\n",
    "  \"isnt\": \"is not\",\n",
    "  \"itd\": \"it had\",\n",
    "  \"itdve\": \"it would have\",\n",
    "  \"itll\": \"it will\",\n",
    "  \"itllve\": \"it will have\",\n",
    "  \"its\": \"it is\",\n",
    "  \"lets\": \"let us\",\n",
    "  \"maam\": \"madam\",\n",
    "  \"maynt\": \"may not\",\n",
    "  \"mightve\": \"might have\",\n",
    "  \"mightnt\": \"might not\",\n",
    "  \"mightntve\": \"might not have\",\n",
    "  \"mustve\": \"must have\",\n",
    "  \"mustnt\": \"must not\",\n",
    "  \"mustntve\": \"must not have\",\n",
    "  \"neednt\": \"need not\",\n",
    "  \"needntve\": \"need not have\",\n",
    "  \"oclock\": \"of the clock\",\n",
    "  \"oughtnt\": \"ought not\",\n",
    "  \"oughtntve\": \"ought not have\",\n",
    "  \"shant\": \"shall not\",\n",
    "  \"shant\": \"shall not\",\n",
    "  \"shantve\": \"shall not have\",\n",
    "  \"shed\": \"she would\",\n",
    "  \"shedve\": \"she would have\",\n",
    "  \"shell\": \"she will\",\n",
    "  \"shellve\": \"she will have\",\n",
    "  \"shes\": \"she is\",\n",
    "  \"shouldve\": \"should have\",\n",
    "  \"shouldnt\": \"should not\",\n",
    "  \"shouldntve\": \"should not have\",\n",
    "  \"sove\": \"so have\",\n",
    "  \"sos\": \"so is\",\n",
    "  \"thatd\": \"that would\",\n",
    "  \"thatdve\": \"that would have\",\n",
    "  \"thats\": \"that is\",\n",
    "  \"thered\": \"there had\",\n",
    "  \"theredve\": \"there would have\",\n",
    "  \"theres\": \"there is\",\n",
    "  \"theyd\": \"they would\",\n",
    "  \"theydve\": \"they would have\",\n",
    "  \"theyll\": \"they will\",\n",
    "  \"theyllve\": \"they will have\",\n",
    "  \"theyre\": \"they are\",\n",
    "  \"theyve\": \"they have\",\n",
    "  \"tove\": \"to have\",\n",
    "  \"wasnt\": \"was not\",\n",
    "  \"wed\": \"we had\",\n",
    "  \"wedve\": \"we would have\",\n",
    "  \"well\": \"we will\",\n",
    "  \"wellve\": \"we will have\",\n",
    "  \"were\": \"we are\",\n",
    "  \"weve\": \"we have\",\n",
    "  \"werent\": \"were not\",\n",
    "  \"whatll\": \"what will\",\n",
    "  \"whatllve\": \"what will have\",\n",
    "  \"whatre\": \"what are\",\n",
    "  \"whats\": \"what is\",\n",
    "  \"whatve\": \"what have\",\n",
    "  \"whens\": \"when is\",\n",
    "  \"whenve\": \"when have\",\n",
    "  \"whered\": \"where did\",\n",
    "  \"wheres\": \"where is\",\n",
    "  \"whereve\": \"where have\",\n",
    "  \"wholl\": \"who will\",\n",
    "  \"whollve\": \"who will have\",\n",
    "  \"whos\": \"who is\",\n",
    "  \"whove\": \"who have\",\n",
    "  \"whys\": \"why is\",\n",
    "  \"whyve\": \"why have\",\n",
    "  \"willve\": \"will have\",\n",
    "  \"wont\": \"will not\",\n",
    "  \"wontve\": \"will not have\",\n",
    "  \"wouldve\": \"would have\",\n",
    "  \"wouldnt\": \"would not\",\n",
    "  \"wouldntve\": \"would not have\",\n",
    "  \"yall\": \"you all\",\n",
    "  \"yalls\": \"you alls\",\n",
    "  \"yalld\": \"you all would\",\n",
    "  \"yalldve\": \"you all would have\",\n",
    "  \"yallre\": \"you all are\",\n",
    "  \"yallve\": \"you all have\",\n",
    "  \"youd\": \"you had\",\n",
    "  \"youdve\": \"you would have\",\n",
    "  \"youll\": \"you you will\",\n",
    "  \"youllve\": \"you you will have\",\n",
    "  \"youre\": \"you are\",\n",
    "  \"youve\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eumZEV2Jju7B"
   },
   "outputs": [],
   "source": [
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "def replace_emoticon_happy(text):\n",
    "    pattern = re.compile(r'(?::|;|=)(?:-)?(?:\\)|D|P)')\n",
    "    return pattern.sub(r'happy', text)\n",
    "def replace_emoticon_sad(text):\n",
    "    pattern = re.compile(r'([:><].?-?[@><cC(\\[{\\|]\\|?|[D][:8;=X]<?|v.v|:/|:\\\\|:-\\\\|:-/)')\n",
    "    return pattern.sub(r'sad', text)\n",
    "def reduce_length(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUExZB5TibFT"
   },
   "outputs": [],
   "source": [
    "## manually generated list of stop words so that negation words are not removed when training the model\n",
    "STOPWORDS = [\"before\",\"more\",\"ll\",\"about\",\"you'd\",\"because\",\"from\",\"we\",\"you'll\",\"through\",\n",
    " \"were\",\"up\",\"re\",\"all\",\"do\",\"your\",\"where\",\"am\",\"won\",\"mightn\",\"it's\",\"it\",\"when\",\n",
    " \"yourself\",\"that\",\"above\",\"y\",\"of\",\"than\",\"by\",\"haven\",\"between\",\"over\",\"has\",\"hers\",\n",
    " \"itself\",\"too\",\"most\",\"she's\",\"once\",\"ve\",\"few\",\"off\",\"that'll\",\"his\",\"m\",\"as\",\"how\",\n",
    " \"are\",\"have\",\"she\",\"there\",\"ours\",\"doing\",\"for\",\"such\",\"and\",\"ma\",\"again\",\"own\",\"will\",\n",
    " \"ourselves\",\"whom\",\"its\",\"you've\",\"an\",\"they\",\"you're\",\"while\",\"some\",\"these\",\"so\",\"on\",\n",
    " \"themselves\",\"if\",\"a\",\"shan\",\"is\",\"this\",\"very\",\"their\",\"now\",\"at\",\"then\",\"t\",\"or\",\"each\",\"d\",\"i\",\n",
    " \"into\",\"which\",\"had\",\"our\",\"did\",\"him\",\"was\",\"what\",\"yours\",\"be\",\"does\",\"herself\",\"me\",\"those\",\"them\",\n",
    " \"s\",\"yourselves\",\"here\",\"to\",\"myself\",\"both\",\"weren\",\"himself\",\"you\",\"been\",\"the\",\"during\",\"but\",\"o\",\n",
    " \"in\",\"other\",\"who\",\"after\",\"her\",\"why\",\n",
    " \"being\",\"he\",\"having\",\"can\",\"out\",\"just\",\"with\",\"my\",\"further\",\"any\",\"until\",\"theirs\",\"same\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m65QxOs0SqnC"
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "# reset the index\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "## regex to remove the special characters\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "## regex to remove other special or bad symbols\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # replacing the happy emoticons with the word happy\n",
    "    text = replace_emoticon_happy(text)\n",
    "    # replacing the sad emoticons with sad word\n",
    "    text = replace_emoticon_sad(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = expandContractions(text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    text = reduce_length(text)\n",
    "    text = ' '.join([Word(word).lemmatize() for word in text.split()])\n",
    "    return text\n",
    "#%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below code calls the above function for cleaning and preprocessing the text...takes about 8 min..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sZQGOicun9LI",
    "outputId": "6796eca1-f0b6-4bcb-b3ae-0b04a32ac082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 1s, sys: 1.68 s, total: 9min 3s\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_full['text'] = df_full['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['totalwords'] = df_full['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_full['totalwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full['totalwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.68212"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_full['totalwords'])/len(df_full['totalwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexical_diversity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.68212"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full['totalwords'] = df_full['text'].str.split().str.len()\n",
    "\n",
    "print(\"lexical_diversity\")\n",
    "sum(df_full['totalwords'])/len(df_full['totalwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the model paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0y6E_wq1SqnD"
   },
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e29PeCqfSqnF",
    "outputId": "1a16afd7-c8b8-4df8-feda-2eeaf52086db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 529317 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_full['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data and train label padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cVW2gPWlSqnI",
    "outputId": "8edb9d57-76e6-4ba2-d58d-d075c432a5ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (650000, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df_full['text'].values)\n",
    "X = pad_sequences(X, maxlen=250)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "30AvmM_dSqnK",
    "outputId": "03ddc6cf-00de-4b8c-948e-14449f633e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (650000, 5)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df_full['label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "P1BT32xbyz7J",
    "outputId": "9a8228c4-95e2-4236-c1c7-87ad15524b8e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "6z70Ec91SqnR",
    "outputId": "d311e72b-15b9-43d7-bd7b-a0b738cdf689",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 250, 250)          551000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 250)               501000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 4,053,255\n",
      "Trainable params: 4,053,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "## add the embedding layer to the model\n",
    "model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))\n",
    "## initialise drop out parameter\n",
    "model.add(Dropout(0.4))\n",
    "# initialising the first layer of LSTM\n",
    "model.add(LSTM(250, return_sequences=True,dropout=0.3, recurrent_dropout=0.3))\n",
    "# initialising the second layer of LSTM\n",
    "model.add(LSTM(250, dropout=0.2, recurrent_dropout=0.2))\n",
    "# initialising the output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "## compile the model for loss fucntion optimizer and metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#check summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below error is because the model was stopped midway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "JbAu4MaoF-lk",
    "outputId": "9b06adb7-c59f-4279-dbf8-28672a49487f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552500 samples, validate on 97500 samples\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a41e44b7cb42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "batch_size = 400\n",
    "history = model.fit(X, Y, epochs=epochs, batch_size=batch_size,validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umfJw7ZKSqnW"
   },
   "outputs": [],
   "source": [
    "# saving the model for future production\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing the test data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjP6EcUJSqne"
   },
   "outputs": [],
   "source": [
    "#new_complaint = test_data['text']\n",
    "test_data['text'] = test_data['text'].apply(clean_text)\n",
    "test_data['text'] = test_data['text'].str.replace('\\d+', '')\n",
    "new_complaint = test_data['text']\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the sequence and predicting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "TbGh_HascDok",
    "outputId": "1711bdb4-f846-42e8-efef-904f23360115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1780254e-01 3.6738023e-01 3.1277019e-01 8.8856846e-02 1.3190233e-02]\n",
      " [9.9011930e-05 3.0360518e-03 3.1540257e-01 6.4054340e-01 4.0918931e-02]\n",
      " [7.7738792e-01 2.1349868e-01 8.6008180e-03 3.8542200e-04 1.2716002e-04]\n",
      " ...\n",
      " [2.6421753e-01 6.1859310e-01 1.1584746e-01 1.2135351e-03 1.2841354e-04]\n",
      " [1.6019880e-04 1.0199553e-04 1.2862789e-04 4.7241622e-03 9.9488503e-01]\n",
      " [1.8277639e-03 1.1277750e-01 7.7003568e-01 1.1249510e-01 2.8639394e-03]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(seq, maxlen = 250)\n",
    "pred = model.predict(padded)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the class label for test data using the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSUMh2eqSqnh"
   },
   "outputs": [],
   "source": [
    "predicted = np.argmax(pred, axis=1)\n",
    "## add one to all as python index starts from 0\n",
    "new_list = [x+1 for x in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GW8aqrD5Sqni"
   },
   "outputs": [],
   "source": [
    "# setting the predicted dataframe\n",
    "predictions = pd.DataFrame(data=new_list, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6rdzUsZSqnj"
   },
   "outputs": [],
   "source": [
    "# getting the labels from test data\n",
    "predictions['test_id'] = test_data['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtXRVBlQSqnl"
   },
   "outputs": [],
   "source": [
    "# write the csv file to disk/drive\n",
    "predictions.to_csv('gdrive/My Drive/FIT5149_Assignment_2/Assignment2/predicted_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7p6QuzWSqnp"
   },
   "outputs": [],
   "source": [
    "padd"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rnn_code.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
